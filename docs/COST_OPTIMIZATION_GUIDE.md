# 비용 최적화 가이드

**작성일**: 2025년 12월 15일

---

## 💰 비용 분석

### OpenAI 임베딩 모델 비용 (2024-2025)

| 모델 | 일반 가격 | 배치 가격 | 차원 | 비용 효율성 |
|------|-----------|-----------|------|-------------|
| text-embedding-3-small | $0.02/1M | **$0.01/1M** ✅ | 1536 | **최고** |
| text-embedding-3-large | $0.13/1M | $0.065/1M | 3072 | 낮음 |

**결론**: `text-embedding-3-small` 사용 시 **배치 처리로 비용 50% 절감** ($0.02 → $0.01)

---

## ✅ 적용된 비용 최적화

### 1. 배치 처리 (비용 50% 절감)

#### 임베딩 생성
- ✅ **배치 크기: 100** (OpenAI 최대 배치 크기)
- ✅ 배치 처리 시: **$0.01/1M tokens** (일반: $0.02/1M tokens)
- ✅ **비용 절감: 50%**

#### 벡터 업로드
- ✅ 배치 크기: 100
- ✅ 배치 간 지연: 1초 (API 제한 고려)

### 2. 캐싱 전략 (중복 호출 방지)

#### 임베딩 캐싱
- ✅ Redis 캐시 (1시간 TTL)
- ✅ 동일 텍스트 재사용 시 API 호출 생략
- ✅ **예상 비용 절감: 30-50%** (캐시 히트율에 따라)

#### 검색 결과 캐싱
- ✅ Redis 캐시 (30분 TTL)
- ✅ 동일 쿼리 재검색 시 즉시 반환

#### 쿼리 분류 캐싱
- ✅ Redis 캐시 (1시간 TTL)
- ✅ 동일 쿼리 분류 로직 스킵

### 3. 적응형 청킹 (청크 수 최소화)

#### 최적 청크 크기
- ✅ **512 토큰** (약 2000자)
- ✅ 문장 경계 인식으로 불필요한 분할 방지
- ✅ **청크 수 최소화 = 임베딩 생성 비용 절감**

#### 청크 오버랩
- ✅ **15% 오버랩** (최적 밸런스)
- ✅ 너무 작으면: 컨텍스트 손실
- ✅ 너무 크면: 검색 정확도 저하

### 4. 모델 선택 최적화

#### 간단한 질문: SLM 사용 (무료)
- ✅ Ollama 로컬 모델 사용
- ✅ 비용: **$0** (로컬 실행)
- ✅ 복잡도 임계값: 0.7 이하

#### 복잡한 질문: LLM 사용 (유료)
- ✅ 비용 효율적인 모델 우선
- ✅ 우선순위: GPT-4o-mini > GPT-4o > Claude > Gemini

---

## 📊 비용 추정

### 시나리오 1: 뉴스 1000개 인덱싱

**가정**:
- 평균 뉴스 길이: 2000자 (1 토큰 ≈ 4자)
- 평균 청크 수: 2개 (적응형 청킹)
- 총 토큰 수: 1000 × 2 × 500 = 1,000,000 tokens

**비용 계산**:
- 일반 처리: 1M tokens × $0.02 = **$0.02**
- 배치 처리: 1M tokens × $0.01 = **$0.01** ✅
- **절감: $0.01 (50%)**

### 시나리오 2: 일일 검색 10,000회

**가정**:
- 평균 쿼리 길이: 50자 (12.5 토큰)
- 캐시 히트율: 40%
- 총 토큰 수: 10,000 × 0.6 × 12.5 = 75,000 tokens

**비용 계산**:
- 캐시 없음: 75,000 tokens × $0.02 = **$0.0015**
- 캐시 있음: 75,000 tokens × 0.6 × $0.02 = **$0.0009** ✅
- **절감: $0.0006 (40%)**

---

## 🎯 비용 최적화 전략

### 1. 배치 처리 최대화

```python
# ❌ 비효율적: 개별 호출
for text in texts:
    embedding = create_embedding(text)  # $0.02/1M tokens

# ✅ 효율적: 배치 처리
embeddings = create_embeddings_batch(texts)  # $0.01/1M tokens (50% 절감)
```

### 2. 캐싱 강화

```python
# ✅ 캐시 사용
embedding = create_embedding(text, use_cache=True)  # 캐시 히트 시 비용 $0
```

### 3. 적응형 청킹

```python
# ❌ 비효율적: 고정 크기
chunks = chunk_text(text, chunk_size=500)  # 많은 청크 생성

# ✅ 효율적: 적응형 청킹
chunks = adaptive_chunk(text)  # 최소 청크 수 (비용 절감)
```

### 4. 모델 선택

```python
# ✅ 간단한 질문: SLM (무료)
if complexity == "simple":
    use_slm()  # 비용: $0

# ✅ 복잡한 질문: LLM (유료, 하지만 정확도 필요)
if complexity == "complex":
    use_llm()  # 비용: 있지만 정확도 필요
```

---

## 📈 예상 비용 절감

### 월간 비용 추정

**가정**:
- 뉴스 인덱싱: 10,000개/월
- 검색 쿼리: 100,000회/월
- 평균 뉴스 길이: 2000자
- 평균 쿼리 길이: 50자

**최적화 전**:
- 인덱싱: 10,000 × 2 청크 × 500 토큰 × $0.02 = **$0.20**
- 검색: 100,000 × 12.5 토큰 × $0.02 = **$0.25**
- **총: $0.45/월**

**최적화 후**:
- 인덱싱: 10,000 × 2 청크 × 500 토큰 × $0.01 (배치) = **$0.10** ✅
- 검색: 100,000 × 12.5 토큰 × 0.6 (캐시) × $0.02 = **$0.15** ✅
- **총: $0.25/월**

**절감: $0.20/월 (44% 절감)** 🎉

---

## ✅ 완료된 비용 최적화

- ✅ 배치 처리 구현 (비용 50% 절감)
- ✅ 캐싱 전략 적용 (중복 호출 방지)
- ✅ 적응형 청킹 (청크 수 최소화)
- ✅ 모델 선택 최적화 (SLM 우선 사용)
- ✅ 비용 효율적인 임베딩 모델 선택
- ✅ 환경 변수 기반 설정 (하드코딩 제거)

---

## 🔧 설정 조정

### 비용을 더 절감하려면

1. **캐시 TTL 증가**
   ```python
   EMBEDDING_CACHE_TTL = 7200  # 2시간 (기본: 1시간)
   ```

2. **배치 크기 증가**
   ```python
   EMBEDDING_BATCH_SIZE = 100  # 최대값 (기본: 100)
   ```

3. **SLM 사용 확대**
   ```python
   USE_SLM_FOR_SIMPLE_QUERIES = True
   SLM_THRESHOLD = 0.8  # 더 많은 질문을 SLM으로 처리
   ```

---

**작성자**: AI Assistant  
**프로젝트**: InsightStock AI Service
