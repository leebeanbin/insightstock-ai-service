"""
AI Service Main Server
InsightStock AI Service - LLM/SLM integration with multiple providers (Python)
"""

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import uvicorn
from loguru import logger

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (config/env.pyì—ì„œ ì¤‘ì•™ ê´€ë¦¬)
from src.config.env import EnvConfig
from src.config.managers import get_redis_manager

# Import controllers
from src.controllers.chat_controller import router as chat_router
from src.controllers.search_controller import router as search_router


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    import asyncio
    from src.workers.chat_storage_worker import ChatStorageWorker

    # Startup
    logger.info("ğŸš€ AI Service starting up...")

    # ì‚¬ìš© ê°€ëŠ¥í•œ Provider í™•ì¸
    from src.providers import ProviderFactory

    available_providers = ProviderFactory.get_available_providers()
    logger.info(f"Available LLM providers: {available_providers}")

    if not available_providers:
        logger.warning("âš ï¸  No LLM providers available! Please set at least one API key.")
    else:
        # ê¸°ë³¸ Provider í™•ì¸
        try:
            default_provider = ProviderFactory.get_default_provider()
            logger.info(f"âœ… Default provider: {default_provider.name}")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize default provider: {e}")

    # ì›Œì»¤ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬, ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›)
    from src.config.env import EnvConfig
    
    worker = ChatStorageWorker()
    worker_task = None
    
    try:
        batch_size = EnvConfig.WORKER_BATCH_SIZE
        worker_task = asyncio.create_task(worker.run(batch_size=batch_size))
        logger.info(f"âœ… Chat storage worker started (batch_size={batch_size})")
    except Exception as e:
        logger.warning(f"âš ï¸  Failed to start chat storage worker: {e}")

    yield

    # Shutdown
    logger.info("ğŸ›‘ AI Service shutting down...")
    
    # ì›Œì»¤ ì¤‘ì§€
    if worker_task:
        try:
            worker.stop()
            worker_task.cancel()
            try:
                await worker_task
            except asyncio.CancelledError:
                pass
            logger.info("âœ… Chat storage worker stopped")
        except Exception as e:
            logger.warning(f"âš ï¸  Error stopping worker: {e}")
    
    ProviderFactory.clear_cache()
    redis_manager = get_redis_manager()
    redis_manager.close()  # Redis ì—°ê²° ì¢…ë£Œ


# Initialize FastAPI app
app = FastAPI(
    title="InsightStock AI Service",
    description="AI Service for LLM/SLM integration with multiple providers (OpenAI, Claude, Ollama, Gemini)",
    version="1.0.0",
    lifespan=lifespan,
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Register routes
app.include_router(chat_router, prefix="/api", tags=["chat"])
app.include_router(search_router, prefix="/api", tags=["search"])


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    from src.providers import ProviderFactory

    available_providers = ProviderFactory.get_available_providers()

    return {
        "status": "ok",
        "service": "ai-service",
        "version": "1.0.0",
        "available_providers": available_providers,
    }


if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ ê²€ì¦
    missing = EnvConfig.validate()
    if missing:
        logger.warning(f"âš ï¸  Missing environment variables: {', '.join(missing)}")
        logger.warning("Service will start but may not function correctly.")

    logger.info(f"ğŸš€ Starting server on {EnvConfig.HOST}:{EnvConfig.PORT}")
    logger.info(f"ğŸ“– API Documentation: http://{EnvConfig.HOST}:{EnvConfig.PORT}/docs")

    uvicorn.run(
        "src.main:app",
        host=EnvConfig.HOST,
        port=EnvConfig.PORT,
        reload=True,
        log_level=EnvConfig.LOG_LEVEL.lower(),
    )
